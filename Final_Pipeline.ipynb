{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_Pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vkvWkTZxol0"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import models\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab.patches import cv2_imshow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "letters_model = models.load_model(\"/content/Letters_model.h5\")\n",
        "emogi_model = models.load_model(\"/content/Emogis_model (1).h5\")"
      ],
      "metadata": {
        "id": "rEzHytk-2-l_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emogi_map = {0: 1, 1: 2, 2: 3, 3: 4, 4: 5, 5: 6, 6: 7}\n",
        "\n",
        "letters_map = {0: 'H', 1: 'I', 2: 'J', 3: 'K', 4: 'N', 5: 'T', 6: 'W', 7: 'X', 8: 'Y', 9: 'Z', 10: 'f', 11: 'r', 12: 't'}"
      ],
      "metadata": {
        "id": "Rz0sX4A0mwAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dir = \"/content/input_dir\"\n",
        "output_dir = \"/content/output_dir\""
      ],
      "metadata": {
        "id": "WmqBqWDacKAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_border(img):\n",
        "    white = [255,255,255]\n",
        "    _, img = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)\n",
        "    constant= cv2.copyMakeBorder(img,30,30,30,30,cv2.BORDER_CONSTANT,value=white)\n",
        "    return constant"
      ],
      "metadata": {
        "id": "c_nYaBOJ0DaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sharpen_image(img):\n",
        "    kernel = np.ones((3, 3),np.uint8)\n",
        "    erosion = cv2.morphologyEx(img, cv2.MORPH_ERODE, kernel, iterations = 2)\n",
        "    #cv2.imshow('erosion', erosion )\n",
        "\n",
        "    _, thresh1 = cv2.threshold(erosion, 127, 255, cv2.THRESH_BINARY)\n",
        "    blur = cv2.blur(thresh1, (2, 2))\n",
        "    #cv2.imshow('blur', blur)\n",
        "\n",
        "    _, thresh2 = cv2.threshold(blur, 145, 255, cv2.THRESH_BINARY)\n",
        "    #cv2.imshow('Thresh', thresh2)\n",
        "\n",
        "    #kernel1 = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(2,2))\n",
        "    final = cv2.filter2D(thresh2, -1, kernel)\n",
        "    return final"
      ],
      "metadata": {
        "id": "IBzeiK8lyAsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def shorten_image(img):\n",
        "    #img = cv2.imread(img, cv2.IMREAD_GRAYSCALE)\n",
        "    height, width = img.shape\n",
        "    _, thresh = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)\n",
        "    horizontal_projection = np.sum(thresh, axis=1)\n",
        "    horizontal_projection = horizontal_projection/255\n",
        "    blankImage = np.zeros_like(img)\n",
        "    for i, value in enumerate(horizontal_projection):\n",
        "        cv2.line(blankImage, (0, i), (width-int(value), i), (255, 255, 255), 1)\n",
        "\n",
        "    size = horizontal_projection.size\n",
        "    a = []\n",
        "    for i in range(size-1):\n",
        "        if horizontal_projection[i] == 0 and horizontal_projection[i+1] != 0:\n",
        "            a.append(i)\n",
        "        elif horizontal_projection[i] != 0 and horizontal_projection[i+1] == 0:\n",
        "            a.append(i)\n",
        "    \n",
        "    if len(a) > 0:\n",
        "     return thresh[a[0]:a[len(a)-1]]\n",
        "    return a"
      ],
      "metadata": {
        "id": "UNIjTdQPViTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image(input, output_dir):\n",
        " img = cv2.imread(input)\n",
        " scale_percent = 60 # percent of original size\n",
        " width = int(img.shape[1] * scale_percent / 100)\n",
        " height = int(img.shape[0] * scale_percent / 100)\n",
        " dim = (width, height)\n",
        " img = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
        " height, width, _ = img.shape\n",
        " img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        " ret, thresh = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY_INV)\n",
        " #cv2.imshow('Thresh', thresh)\n",
        " vertical_projection = np.sum(thresh, axis=0)\n",
        " vertical_projection = vertical_projection/255\n",
        " #print(vertical_projection[0])\n",
        " blankImage = np.zeros_like(img)\n",
        " for i, value in enumerate(vertical_projection):\n",
        "     #print(value)\n",
        "     cv2.line(blankImage, (i, 0), (i, height-int(value)), (255, 255, 255), 1)\n",
        " size = vertical_projection.size\n",
        " a = []\n",
        " for i in range(size-1):\n",
        "     if vertical_projection[i] == 0 and vertical_projection[i+1] != 0:\n",
        "         a.append(i)\n",
        "     elif vertical_projection[i] != 0 and vertical_projection[i+1] == 0:\n",
        "         a.append(i)\n",
        " #print(a)\n",
        " #cv2.imshow(\"New Histogram Projection\", blankImage)\n",
        " s = 0\n",
        " for i in range(0, len(a)-1, 2):\n",
        "     img_convert = thresh[:, a[i]:a[i+1]]\n",
        "     img_convert = shorten_image(img_convert)\n",
        "     if len(img_convert) != 0:\n",
        "      h, w = img_convert.shape\n",
        "      x = h*w\n",
        "      if x>10:\n",
        "       _, img_convert = cv2.threshold(img_convert, 127, 255, cv2.THRESH_BINARY_INV)\n",
        "       img_convert = make_border(img_convert)\n",
        "       img_convert = sharpen_image(img_convert)\n",
        "       img_convert = cv2.resize(img_convert, (28, 28))\n",
        "      #  img_convert = imutils.resize(img_convert, height = 28, width = 28)\n",
        "       #cv2.imwrite(str(s)+\".jpg\",img_convert)\n",
        "       cv2.imwrite(os.path.join(output_dir, str(s)+'.jpg'), img_convert)\n",
        "       #img_convert = cv2.cvtColor(img_convert, cv2.COLOR_BGR2GRAY)\n",
        "       # _, Thresh = cv2.threshold(img_convert, 127, 255, cv2.THRESH_BINARY_INV)\n",
        "       # cv2.imshow(str(s), Thresh)\n",
        "       s += 1\n",
        "\n",
        " #cv2.imshow(\"solon\", vert_image[0])\n",
        " #cv2.waitKey(0)\n",
        " #cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "-kM0IC8wbvJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def char_in_image(file_path):\n",
        "  img = tf.keras.utils.load_img(\n",
        "        file_path,\n",
        "        grayscale=False,\n",
        "        color_mode='grayscale',\n",
        "        target_size=(28,28,1),\n",
        "        interpolation='nearest'\n",
        "  )\n",
        "  img_array = tf.keras.utils.img_to_array(img)\n",
        "  # print(img_array.shape)\n",
        "  img_array = img_array.reshape(-1,28, 28, 1)\n",
        "  # img_array = img_array/255.0\n",
        "  # img_array_1 = img_array.astype('float32')/255.\n",
        "  # img_array_1 = tf.expand_dims(img_array_1, 0)\n",
        "  # img_array = tf.expand_dims(img_array, 0)\n",
        "  letter_predictions = letters_model.predict((img_array.astype('float32'))/255.).flatten(order='C')\n",
        "  emogi_predictions = emogi_model.predict((img_array.astype('float32'))/255.).flatten(order='C')\n",
        "  # print(emogi_map[emogi_predictions.argmax()],\" \" , letters_map[letter_predictions.argmax()])\n",
        "  # print(letter_predictions)\n",
        "  # print(emogi_predictions)\n",
        "  if(max(emogi_predictions)>max(letter_predictions) and max(letter_predictions)<=0.85):\n",
        "      print(emogi_map[emogi_predictions.argmax(axis=0)])\n",
        "  else:\n",
        "      print(letters_map[letter_predictions.argmax(axis=0)])"
      ],
      "metadata": {
        "id": "ls6OK-VjwYkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for filename in os.listdir(input_dir):\n",
        "    f = os.path.join(input_dir, filename)\n",
        "    # checking if it is a file\n",
        "    if os.path.isfile(f):\n",
        "        preprocess_image(input = f, output_dir = output_dir)\n",
        "# in input, specify path to image\n",
        "# in output, specify the directory where the images have to be saved in colab workspace"
      ],
      "metadata": {
        "id": "UrGLyxxgbxft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for filename in os.listdir(output_dir):\n",
        "    f = os.path.join(output_dir, filename)\n",
        "    # checking if it is a file\n",
        "    if os.path.isfile(f):\n",
        "        img = cv2.imread(f)\n",
        "        cv2_imshow(img)\n",
        "        char_in_image(f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 801
        },
        "id": "RrCTzrTCbzpL",
        "outputId": "540654ab-c75a-4726-f233-af157e975ca8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=28x28 at 0x7F1369569CD0>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAABUklEQVR4nNVVMZKEMAyTnfC5fQIlNX/jdxQE4is0+LKBu4OwzbnYgWwiS3JsxMzQGjlnVT2uy13QnLOIAODvaZzk+T1UlXDOJqX0FBTAuq4i4lhd11Ubbss3s1J49cq4zVREzCznjN1fPn+SKS8AM7UzLeMUEUBsgzMzM1PVvu+P/96WT4JmFkLAbnHlSQvo9+G9aFVf3faUtSY1N6Ha09JRAIZheL1eJfSbgjb5ZcW3baO/7UynaXLEsmKPmJbldo5V9dVBj93G3b7OpuR+QjjHqv2VCb2IZrYsiyOKiKqSWozxoqw3+YQuV8iOYWbHmpyGlkXkSdfIHPM8hxC4cgURgGzbhv32eW+oqlupqu7P6fQ8YepW8mVdVwDLsnD8cFzygVKuMFUAftgFxhhTSuM4Mhnh3IS/5fulc+0pJX52WBZ3+SJNPJxSP8Wjyf//Qb8A6F8yGpIjur4AAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4   W\n",
            "[1.5496500e-02 3.8409192e-04 3.4261093e-01 9.9922833e-04 4.0672496e-02\n",
            " 7.0025388e-04 5.0669032e-01 1.1675908e-04 7.8294039e-02 5.3235106e-03\n",
            " 8.2472824e-03 8.6758548e-05 3.7789161e-04]\n",
            "[3.2767440e-07 1.6848241e-01 1.0884382e-04 8.2699531e-01 7.4683078e-07\n",
            " 2.3087594e-03 2.1034929e-03]\n",
            "4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=28x28 at 0x7F13694309D0>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAACOUlEQVR4nN1VO64qMQx1PBHShYIWUSCxEJZBQwVsgW2xC3oWQEVNAxVoNHF8izOcl5n3YW77UowcJz728Th2cHcRyTmrqruHEKCBUG55TT4tFREzU9WcMyyxsAVuCMHM3H0IYgsKLHyJoqpN0zBAKIcgikjLzsyqqoKqJyO6EMJA7iKiZiYiVVVBACKIM0DmYWCkyqAgqOrX15e7V1WlqqqaUmJaQOszKAywyTnHGF+vV86ZyuVyqaqz2ewHwbo7mLr74XCgjKOUErb4+5A/LiltQLY9eDPgKUHNbLVaQYOj3W7XAeVtFqO7N00jIrfb7XdEytfrtXTfATWzki+3MUZkGUkgNFzyPuvkD5G6+2Kx6Bx0/TPFZdQiUjrrgOaccRtf3Nvv96WyzIOZQe5l/BcoIy8pgCA1qgpLOID+dDr9LVjtFVbOGQ+BrEWkrmv0sKqqUkqo1vl8DsPpdAorrihFE4GKb5+NxotmGGM0sxgj6W82m35PIKnj8ViyA6nL5UJSpMkiZX56yRXvPiEs/pzxeNxLGS4j6b2fxlcnpTfIdV3jbDKZ9Cofuev5LvlB0BACa/t+v7v7aDTCAzufz0gWqazX6/KflKnn/3D3wBhVFZ24bV/vAVOOLxrDDUotxphSQhdubdGbYebuz+cTs4QccY/lAa+qCpQYIxomWkcLyiYN7Xa7dfflckkgYDVN83g8Ukrwh4puM/iuJ4yPPn0SN7NyhKSUUJj/nisEGTohfrQGTcf/F/Qb3hUxMgg/vQUAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5   W\n",
            "[1.09977111e-06 9.64445429e-13 1.71432628e-08 4.98451556e-08\n",
            " 2.77609928e-07 2.95369257e-10 9.99986529e-01 1.32616085e-10\n",
            " 1.20201039e-05 1.81502802e-09 1.32264885e-11 1.22880062e-09\n",
            " 1.55882390e-11]\n",
            "[0.0000000e+00 1.4837981e-15 1.9533313e-07 5.3607924e-17 9.9679071e-01\n",
            " 3.2087974e-03 2.6466176e-07]\n",
            "W\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=28x28 at 0x7F13693AE050>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAABuElEQVR4nN1VMZLCMAyULGdS8gcewZOY4WM8gJYH0POUUIEj64rlhEliJ3PlqUpia727khw2MyJSVRExM2YmIn9ohJmZWQgh5xxCKJcClkVEVQGUUmoj5pyJiJmBxcxg9gEFKeBireu6NsEQgqMsMnjTZuacMzOras4ZXBaDmcdx9Neu65D4tQef3JctbtKvA9A3z2L4DQe8StBYkw++TnxiKHmhsPx8PqmoQMMBkL3f78ycUprgsjviTP21gWtmj8djt9stbgulEE+A/Ov1ihx8KU9iZiAuljTWuIzjOAxDjDGlBDfKAen7fhgGIiqXPvJrMpGcUur7Hky9Q1Q1xugVniCS92kt0IZIPp1OqmpmQGxkVeXDYmeBoYgx4iRVDSGgwUVkMv4t+d6tYIpMTJTrXRyWKmiJ7qPhrQ4FOAYNsInpnAgQy3LNhSNWCuWIl8vlnfALAQcWZ6/K1KX5pTm/jWpRZepmEdH5fKbZpdeIFabvTd9urka1T3G9m9ntdtvv9z7+m8LWgoiOxyOeccutxnqfisjr9RKRlFL79/VR2VBARMx8OBzQjxsRactE/SE2VfP/gv4AgMDQGqiLfbkAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7   W\n",
            "[1.8981175e-02 4.3849459e-06 2.7213559e-01 4.5433952e-04 1.0389218e-01\n",
            " 8.1807790e-05 4.8437771e-01 1.0104601e-06 1.1924529e-01 4.7092600e-04\n",
            " 2.9163304e-04 5.6080386e-05 7.8435733e-06]\n",
            "[0.0000000e+00 9.6217911e-15 3.4901095e-19 8.9164942e-18 2.0372713e-14\n",
            " 9.0954484e-17 1.0000000e+00]\n",
            "7\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=28x28 at 0x7F13693AEB10>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAACPklEQVR4nN1VOw4qMQx0kl16JGpKToWE6JEQghKJGokr0MAVOAKnoOMYsHH8ioHBZJFe86rnYjeb+DceOxtKKSEEMwshiIiIqGpKyS9KKTFGvzAzEaFJJTGE0HVdCEFVsQVVeMRmjJGLUgpOoZZz7jsNqhpjZKZIwcz8JiXn3DQN88VnX+3lFB/UpsCglGJmKIWZIaRXqDNFat4eSlgzXzxZ5ZxzCIFh6vqamaoiftd19i1U3Ww2q9WKn9vt1hvWVjiLMdIAkQEQhJCTpmlY3LZtiZIIXuyjWAD72Y2RtQN2EfFmKSXgGA6HpRR/JGgpOoLflBIDpJR8Y1bVL6V0XXe73XxCL8Z9dcAGS0wvFOhPp9MQAgBVCmb2cuqZ8Yw1TTObzR6PR20mcjqdELXv9wMNzQQgaGkzyzmfz+fBYEAzj5S81UJjpIkFnkyq2gE4kI5O2O12v+Ejhcreh3w+n1yj7r5PvuAjO3EjgXapOkxV27ZF92CcmAF7jvKZS/hFFuw7uPYXgrkxwxz3nUbf9sDiXVwul/v9XvWpfE8dR+4HUap6PB7JA4Xp86hPAyr2IQqFp568rxjuo+L0yyMzm81mk8lkPB5fr9cvp3ipKkfLtyTt/aRB/B1YsS8YYZ/sfr8HCX0QlOVyidOfXfg1XqqKAZ3P5x5yv/PtTVQppX+rStd1BF4HFBGR9Xpt79sA9uD6cDhArT8sDebX3n86cX+qnHNKabFY+EYejUYkHZrVZSr+H/UPJf5d5X92+ge/YyhzdDw0zAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6   W\n",
            "[4.0298009e-06 1.1495812e-08 2.7767013e-09 7.0830006e-03 4.9142144e-04\n",
            " 1.7125564e-09 9.9232519e-01 7.5051059e-09 2.0244896e-08 1.2521942e-10\n",
            " 1.4011767e-05 8.2392886e-05 6.4331238e-09]\n",
            "[0.0000000e+00 1.2458499e-30 1.1153005e-21 2.2490339e-23 2.4415643e-04\n",
            " 9.9975580e-01 7.6114632e-30]\n",
            "W\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=28x28 at 0x7F1369391490>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAACBElEQVR4nN1VsZECMQyUZB9DAZRHGVxKOZRBA0QUQUhACJblD/Zf6Mwd/Hz4ChifJa9XK0twa42IWmvMTEREVGtNKdGCeSTCzExE4nEiEiJSVWZW1e8tkSVEgLbWzAwX4zMiEhHXWv0qVc05v0GM0N/nmV8zExFxju6rtS7BwcXM4DGrFUf+WECm90yR06ygRCS11ogI8l3Qq0GllNLhcJgJBtNaK+RXVTNDdrNWazWzUoqI7Pd7r1uMYWx1+QL0PdNSCgVxJ+kjXzMjIvCF43q9YgGX2ziOKaUOsYuh1lopJWahqtvtdhxHM8M17kK8b6pq/JykDzpmlnP2J/KqADPf7/ecc6z4a+5EJDgpIhHRWxDe2+0mIpfLJecMCAealZ5Ra68V+ItISgm8WmsQMbYGrqSfnu5Kza5UvL8rK7xLE2SmTV0Xf5vuG4YBNQQXnAQjNChaNqXUtfUzAosov6rudjsoC3bRUkrYpDA0JunPiM0csbwsXa1x5Uz6s4Y0H48HgI7HI01r7YXyoj1tqccpzGPgns9nfHqnQ/Gu8VtriyPZ9cKDQ9HxsJwvZhUzl1KGYficPjKKD+50Oq3Xa7xr9D56lIgiIn0sFI6h2TB3VqsVgPzBop6TLJeY4j+GmZEjjqHNmHmz2XgH+pD7zBRcvCm68YFF/P0t6J/twx/cfwf9AjnKp+CwJUZJAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3   Y\n",
            "[3.9009615e-03 1.7755255e-04 4.5810756e-03 1.4834082e-03 7.5778516e-05\n",
            " 4.7421254e-05 8.3904254e-04 5.4570445e-04 7.0705867e-01 2.6827040e-01\n",
            " 1.2930049e-02 2.1663380e-05 6.8239540e-05]\n",
            "[9.8900981e-33 7.6027646e-08 9.3908095e-01 2.0640516e-09 6.5775849e-03\n",
            " 1.6528319e-06 5.4339688e-02]\n",
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "eOszWANVRtGR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}